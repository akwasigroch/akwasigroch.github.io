<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blog | Arkadiusz Kwasigroch</title>
    <link>https://akwasigroch.github.io/categories/blog/</link>
      <atom:link href="https://akwasigroch.github.io/categories/blog/index.xml" rel="self" type="application/rss+xml" />
    <description>Blog</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Arkadiusz Kwasigroch Â© 2020</copyright><lastBuildDate>Wed, 05 Feb 2020 22:38:09 +0100</lastBuildDate>
    <image>
      <url>https://akwasigroch.github.io/images/icon_huf19dda8888b9106de6dc6f7c8ef8bbc9_17545_512x512_fill_lanczos_center_2.png</url>
      <title>Blog</title>
      <link>https://akwasigroch.github.io/categories/blog/</link>
    </image>
    
    <item>
      <title>Learn probabilistic PCA first to better understand Variational Autoencoders</title>
      <link>https://akwasigroch.github.io/post/blog/probabilistic-pca/</link>
      <pubDate>Wed, 05 Feb 2020 22:38:09 +0100</pubDate>
      <guid>https://akwasigroch.github.io/post/blog/probabilistic-pca/</guid>
      <description>&lt;p&gt;I prepare a presentation on Variational Autoencoders to my university colleagues. I think what can help them understand the topic better. What cause a problem in Variational Autoencoders understanding is a derivation of the loss function as it involves intuition and knowledge of probabilistic concepts.&lt;/p&gt;
&lt;p&gt;I think that understanding probabilistic PCA first helps to understand Variational Autoencoders faster. Probabilistic PCA is a great and simple algorithm. The big advantage of this algorithm is that all the probability distributions used in the algorithm are Gaussian distributions. So starting from the latent variable distribution $p(z)$ and distribution of observed variables conditioned on latent variable $p(x|z)$, we can easily obtain the distribution of observed variable $p(x)$, and posterior distribution $p(z|x)$. It could be obtained analytically by known equations. What&#39;s more, we can easily sample from that distribution using known libraries like Numpy or Scipy. So to gain the intuition for this algorithm, it is worth to play with the examples.&lt;/p&gt;
&lt;p&gt;I prepared the short implementation of the probabilistic PCA method. I plan to show the code to listeners on my presentation to make the algorithm easier to understand. I hope that it also helps you. The code is available at my Github:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/akwasigroch/probabilistic_pca&#34;&gt;https://github.com/akwasigroch/probabilistic_pca&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I plan to prepare longer text on my blog concerning that method.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
